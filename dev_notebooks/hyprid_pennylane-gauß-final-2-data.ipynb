{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75d6aba-26d9-418b-81cc-e9c3b44278a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import time\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = [3, 1]\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d00751-1e31-4d76-90d9-45106f03002b",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d4c06ad-1ad0-430e-b1fe-6dd295f651ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 2000\n",
    "N = 3\n",
    "M = 4\n",
    "num_dim = 1\n",
    "num_discrete_values = 16\n",
    "num_qubits = int(N**2 * np.log2(M))\n",
    "\n",
    "file_name = \"QG_train_normalized_N3_M4_bilinear\"\n",
    "\n",
    "reg = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42594711-6d99-4ccd-a23b-e17bc9422ece",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "698b09f5-c48c-4520-a0ed-96edd2fd4b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(file_name,\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e70763be-ca3c-4204-89c7-d5b2cc0ad1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = f.get('X')\n",
    "y_train = f.get('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f985952e-7458-4280-ab3f-89496e40ff54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 2.5, 2.5, -0.5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADHUlEQVR4nO3YsUkEURhGUWfdAmzESDAxFAULELYEQ6sxM7IJEy3BFkzXBoyEsYFhwn134JxwXvIllx9mmuf5DOjZjR4ALBMnRIkTosQJUeKEqP3a493u0a/cNdeXoxfknX8fR0/Ie/95mZa+u5wQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghar/2+Hd7daodm/Tx9jp6Qt7N89PoCZvlckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR+9XHz69T7dikh/vD6Al5F7/H0RM2y+WEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUdM8z6M3AAtcTogSJ0SJE6LECVHihChxQtQ/2DkWtGmkV2MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.mean(x_train[y_train[:]==1],axis=0),norm=matplotlib.colors.LogNorm())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e519f3e-d862-4be6-aeb7-5f3e2a8e00b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 2.5, 2.5, -0.5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADKUlEQVR4nO3YsUkEURhG0R1Zc1M1MDCwBrECKzESEyuwBhNrsAIxFEzFSgQTEcFnA7MbOnfhnHD+5EsuD2YaY6yAnr2lBwDzxAlR4oQocUKUOCFqve14eXztV+4W32dHS0/I2//4WnpC3tPb3TT33csJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTotbbjp/nJ/+1Yye93D8sPSHv4uZq6Qk7y8sJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRE1jjI3Hy9PbzUdWP4cHS0/Im17fl56Q9/z7OM1993JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqGmMsfQGYIaXE6LECVHihChxQpQ4IUqcEPUHRmQdEoQBvqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.mean(x_train[y_train[:]==0],axis=0),norm=matplotlib.colors.LogNorm())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00244b08-8d19-4b27-a54e-e35de2caceae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 2.5, 2.5, -0.5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADtElEQVR4nO3dwWnDQBBAUcmoClfhJoIrSJWpwLiJVJEysqnAPjl/hfXeVSAGBj5zEGgdYywANE6zBwA4EtEFCIkuQEh0AUKiCxASXYDQ9uzhx+nT92Q7cf/9Wl/1Lnvdj1fudVnsdk8e7dalCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQGibPQDwXm4/37NHWK7ny+wRHnLpAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgNA2ewDgvVzPl9kj7JpLFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoTWMcbsGQAOw6ULEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoS2Zw8/Tp9+oLYT99+v9VXvstf9eOVel8Vu9+TRbl26ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXILTNHoBjuv18zx5huZ4vs0d4S3b7nEsXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChLbZA3BM1/Nl9gj8E7t9zqULEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQusYY/YMAIfh0gUIiS5ASHQBQqILEBJdgJDoAoT+AAfuLYI/q3plAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 3)\n",
    "nums = np.random.randint(0,6,size=(6,))\n",
    "ax[0,0].imshow(x_train[nums[0]])\n",
    "ax[0,0].axis('off')\n",
    "ax[0,1].imshow(x_train[nums[1]])\n",
    "ax[0,1].axis('off')\n",
    "ax[0,2].imshow(x_train[nums[2]])\n",
    "ax[0,2].axis('off')\n",
    "ax[1,0].imshow(x_train[nums[3]])\n",
    "ax[1,0].axis('off')\n",
    "ax[1,1].imshow(x_train[nums[4]])\n",
    "ax[1,1].axis('off')\n",
    "ax[1,2].imshow(x_train[nums[5]])\n",
    "ax[1,2].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06ca5084-3f7b-4795-8fc6-530db9a960da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x_train[y_train[:]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6f241be-277d-4c9a-b817-56b906ff2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c25903c8-54b3-4f5f-b4d3-0bc4a57b8a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3008, 3, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37c535-cac7-441d-b5c4-c178f8c2f155",
   "metadata": {},
   "source": [
    "## Utillity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d6951eb-b4f6-4cf2-a2eb-fc3f38c859c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_bin_list(num,bits):\n",
    "    return [(num>>k)&1 for k in range(0,bits)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8a88da5-8fb1-413d-a96e-7e14c2573f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dim_to_integer(arr,dim):\n",
    "    # Calculate the number of possible values \n",
    "    num_values = 2 ** arr.shape[dim]\n",
    "    arr = torch.flip(arr,(1,))\n",
    "    # Sum along the last dimension to convert it to integers\n",
    "    integer_array = np.sum(arr * (2 ** np.arange(arr.shape[dim])), axis=-1)\n",
    "\n",
    "    return torch.tensor(integer_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60c90e4c-9775-4573-afb9-16c9e2851dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(img):\n",
    "    return img / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f23a767d-5534-4718-aec3-b78178e2134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_array_to_img(basis_array):\n",
    "    basis_array = basis_array.reshape(3,3,2)\n",
    "    return normalize_image(convert_dim_to_integer(basis_array,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f077a577-eb6e-4d0d-a388-63aab01fc46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_basis_array_to_img(batch_basis_array):\n",
    "    batch_basis_array = batch_basis_array.reshape(batch_basis_array.shape[0],3,3,2)\n",
    "    return normalize_image(convert_dim_to_integer(batch_basis_array,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bb9d0d5-b610-4e57-ae8a-5585fa565ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_int(tensor):\n",
    "    tensor = tensor.long()\n",
    "    \n",
    "    # Calculate the number of columns in the tensor\n",
    "    num_columns = tensor.size(1)\n",
    "    \n",
    "    # Create a weight tensor for binary to integer conversion\n",
    "    weights = 2 ** torch.arange(num_columns - 1, -1, -1)\n",
    "    \n",
    "    # Calculate the integer values for each row\n",
    "    int_values = torch.sum(tensor * weights, dim=1)\n",
    "    \n",
    "    return int_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6eb71b05-a7a3-483d-a695-b15dc073bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=20) \n",
    "matplotlib.rc('ytick', labelsize=20) \n",
    "\n",
    "def plot_training_progress(n, samples_tuple):\n",
    "    # we don't plot if we don't have enough data\n",
    "    if len(generator_loss_values) < 2:\n",
    "        return\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.tight_layout(pad=6.0)\n",
    "\n",
    "    # Generator Loss\n",
    "    ax3.plot(generator_loss_values, label=\"generator loss\", color=\"royalblue\")\n",
    "    ax3.plot(discriminator_loss_values, label=\"discriminator loss\", color=\"magenta\")\n",
    "    ax3.legend(loc=\"best\", fontsize=16)\n",
    "    ax3.set_xlabel(\"epoch\", fontsize=16)\n",
    "    ax3.set_ylabel(\"Loss\", fontsize=16)\n",
    "    #ax3.grid()\n",
    "\n",
    "    # Relative Entropy\n",
    "    ax2.plot(entropy_values)\n",
    "    ax2.set_xlabel(\"epoch\", fontsize=18)\n",
    "    ax2.set_ylabel(\"KL divergence\", fontsize=18)\n",
    "    ax2.set_yscale(\"log\")\n",
    "    #ax2.grid()\n",
    "    \n",
    "    paramsg = generator.weights\n",
    "    generated_probabilities = circuit(1,paramsg).detach()\n",
    "    ax1.plot(generated_probabilities, label=\"generated\")\n",
    "    ax1.plot(prob_data,label=\"real\")\n",
    "    ax1.legend(loc=\"upper right\", fontsize=18)\n",
    "    ax1.set_xlabel(\"$i$\", fontsize=18)\n",
    "    ax1.set_ylabel(\"$P_i$\", fontsize=18)\n",
    "    #ax3.grid()\n",
    "    #plt.savefig(f\"imgs/plt-{n}.png\")\n",
    "    \n",
    "    ax4.plot(discriminator(torch.tensor(list(range(16))).reshape(16,1).float()).detach())\n",
    "    ax4.set_xlabel(\"$i$\", fontsize=18)\n",
    "    ax4.set_ylabel(\"$D(i)$\", fontsize=18)\n",
    "    \n",
    "    for samples in samples_tuple:\n",
    "        values, counts = np.unique(samples, return_counts=True)\n",
    "        if (values == np.arange(0,16)).all(): ax5.plot(counts)\n",
    "    ax5.set_xlabel(\"i\")\n",
    "    ax5.set_ylabel(\"sample distribution\")\n",
    "        \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1e02894-a052-4f89-be4d-212a8bca5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size):\n",
    "    for start_idx in range(0, data.shape[0] - batch_size + 1, batch_size):\n",
    "        idxs = slice(start_idx, start_idx + batch_size)\n",
    "        yield data[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d88ba4-b314-4bf9-89f7-2e6b39eb0341",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddec6b-682d-473a-a250-58b25415e129",
   "metadata": {},
   "source": [
    "## Generator circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2be3e809-2454-4a4d-9d30-e6185a625ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit.torch\", wires=18)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\", cachesize=1000000)\n",
    "def circuit(inputs, weights):\n",
    "    for wire in range(18): qml.Hadamard(wires=wire)\n",
    "    qml.StronglyEntanglingLayers(weights=weights, wires=list(range(18)))\n",
    "    return qml.probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "112bd16e-60d3-46d2-adb3-8056d5f50ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sample = qml.device(\"default.qubit\", wires=num_qubits, shots=nbatch)\n",
    "\n",
    "@qml.qnode(dev_sample, interface=\"torch\")\n",
    "def circuit_sample(inputs, weights):\n",
    "    for wire in range(num_qubits): qml.Hadamard(wires=wire)\n",
    "    qml.StronglyEntanglingLayers(weights=weights, wires=list(range(num_qubits)))\n",
    "    return qml.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089dc06-52c0-423d-9b12-aad86e602d93",
   "metadata": {},
   "source": [
    "## Hybrid model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cfee725-a814-4f84-bbe8-4de6ae0e54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers=1\n",
    "weight_shapes = {\"weights\": (n_layers, num_qubits,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "511e3252-b022-4cc8-b2bf-7278878c1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7816b35-0f7d-4312-b68b-4dd943e2f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.linear_input = nn.Linear(input_size, 50)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.linear20 = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        print(input.shape)\n",
    "        x = self.flatten(input)\n",
    "        print(x.shape)\n",
    "        x = self.linear_input(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.linear20(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac8741cf-d61a-4b59-b0cd-701822839634",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = qlayer\n",
    "discriminator = Discriminator(N**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c92289-0d20-4ddd-adec-632d94cf6d8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b36a775e-b67d-498a-a1a0-9d5d84523b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001  # learning rate\n",
    "#b1 = 0.7  # first momentum parameter\n",
    "b1 = 0.6  # first momentum parameter\n",
    "b2 = 0.999  # second momentum parameter\n",
    "\n",
    "generator_optimizer = Adam(generator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0, amsgrad=True)\n",
    "discriminator_optimizer = Adam(\n",
    "    discriminator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0, amsgrad=True\n",
    ")\n",
    "\n",
    "decayRate = 0.9999\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=generator_optimizer, gamma=decayRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "707a5911-94a7-42b3-9683-da3f762f84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_loss(inputs, target, w):\n",
    "    bce_loss = target * torch.log(inputs) + (1 - target) * torch.log(1 - inputs)\n",
    "    weighted_loss = w * bce_loss\n",
    "    total_loss = -torch.sum(weighted_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44271d15-e370-44d0-9796-a496a4c5ab1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94114/1659427866.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutaing discriminator at samples\n",
      "torch.Size([2000, 3, 3])\n",
      "torch.Size([2000, 9])\n",
      "done evaluating discriminator\n",
      "Evaluating Generator\n",
      "done Evaluating Generator\n",
      "Evaluating Generator loss\n",
      "done Evaluating Generator loss\n",
      "Generator loss backwards\n",
      "done Generator loss backwards \n",
      "Generator step \n",
      "done Generator step \n",
      "discriminator from data\n",
      "torch.Size([2000, 3, 3])\n",
      "torch.Size([2000, 9])\n",
      "done discriminator from data\n",
      "discriminator loss\n",
      "done discriminator loss \n",
      "discriminator step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prob_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m discriminator_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     76\u001b[0m discriminator_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 78\u001b[0m entropy_value \u001b[38;5;241m=\u001b[39m entropy(gen_dist\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m+\u001b[39mreg, \u001b[43mprob_data\u001b[49m\u001b[38;5;241m+\u001b[39mreg)\n\u001b[1;32m     79\u001b[0m entropy_values\u001b[38;5;241m.\u001b[39mappend(entropy_value)\n\u001b[1;32m     81\u001b[0m my_lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prob_data' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "\n",
    "torch.nn.init.uniform_(list(generator.parameters())[0], a=-np.pi, b=np.pi)\n",
    "\n",
    "n_epochs = 2000\n",
    "\n",
    "num_qnn_outputs = num_discrete_values**num_dim\n",
    "\n",
    "data = torch.tensor(data, requires_grad=False)\n",
    "\n",
    "generator_loss_values = []\n",
    "discriminator_loss_values = []\n",
    "entropy_values = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    valid = torch.ones(nbatch, dtype=torch.float)\n",
    "    fake = torch.zeros(nbatch, dtype=torch.float)\n",
    "\n",
    "    #samples = tensor_to_int(circuit_sample(1, list(generator.parameters())[0].detach()))\n",
    "    #samples = torch.tensor(np.random.randint(0,16, size=(nbatch,)), dtype=torch.float).reshape(nbatch,1)\n",
    "    samples = torch.tensor(np.random.randint(0,2,size=(nbatch,18)), dtype=torch.float)\n",
    "    samples_idx = convert_dim_to_integer(samples,1)\n",
    "    samples_img = batch_basis_array_to_img(samples)\n",
    "    \n",
    "    #samples = samples.float().reshape(-1,1)\n",
    "    \n",
    "    for j,Xbatch in enumerate(iterate_minibatches(data, batch_size=nbatch)):\n",
    " \n",
    "        # discriminator values\n",
    "        print(\"Evalutaing discriminator at samples\")\n",
    "        disc_value = discriminator(samples_img.float())\n",
    "        print(\"done evaluating discriminator\")\n",
    "    \n",
    "        # gen dist\n",
    "        print(\"Evaluating Generator\")\n",
    "        gen_dist = generator(torch.tensor([]))\n",
    "        print(\"done Evaluating Generator\")\n",
    "        \n",
    "        # Train generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        print(\"Evaluating Generator loss\")\n",
    "        generator_loss = adversarial_loss(disc_value.detach().reshape(-1), valid, gen_dist[samples_idx.long().reshape(-1)])\n",
    "        print(\"done Evaluating Generator loss\")\n",
    "        \n",
    "        # store for plotting\n",
    "        generator_loss_values.append(generator_loss.detach().item())\n",
    "\n",
    "        print(\"Generator loss backwards\")\n",
    "        generator_loss.backward(retain_graph=True)\n",
    "        print(\"done Generator loss backwards \")\n",
    "        print(\"Generator step \")\n",
    "        generator_optimizer.step()\n",
    "        print(\"done Generator step \")\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "     \n",
    "        print(\"discriminator from data\")\n",
    "        real_disc_vals = discriminator(Xbatch.float())\n",
    "        print(\"done discriminator from data\")\n",
    "    \n",
    "        print(\"discriminator loss\")\n",
    "        real_loss = adversarial_loss(real_disc_vals.reshape(-1), valid, torch.tensor(np.ones(nbatch)*1/nbatch, requires_grad=False))\n",
    "        fake_loss = adversarial_loss(disc_value.reshape(-1), fake, gen_dist.detach()[samples_idx.long().reshape(-1)])\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        print(\"done discriminator loss \")\n",
    "        \n",
    "        # Store for plotting\n",
    "        discriminator_loss_values.append(discriminator_loss.detach().item())\n",
    "    \n",
    "        print(\"discriminator step\")\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        entropy_value = entropy(gen_dist.detach().squeeze().numpy()+reg, prob_data+reg)\n",
    "        entropy_values.append(entropy_value)\n",
    "\n",
    "        my_lr_scheduler.step()\n",
    "\n",
    "        plot_training_progress(epoch, (samples,))\n",
    "    break\n",
    "elapsed = time.time() - start\n",
    "print(f\"Fit in {elapsed:0.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d90b160-ee7b-4116-9012-f9cfecd9ae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70a1dc31-6082-48b5-ae98-835d34be84b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_dist.detach()[samples_idx.long().reshape(-1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4acec8-2987-4435-8dbd-1cdf9c4e83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(Xbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0916c-6fa2-41be-bb3e-0ac3ae5e7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsg = generator.weights\n",
    "generated_probabilities = circuit(1,paramsg).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12147f-abc0-4b6e-92c9-5e30db664a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(generated_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015817c-3401-44d9-b842-41d5b1c5fd73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfq",
   "language": "python",
   "name": "tfq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
